{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Recommendation System based on Latent Factor Model (LFM) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This study presents a personalized movie recommendation system leveraging a Latent Factor Model (LFM) to mitigate the decision fatigue faced by consumers amidst a plethora of digital content. Utilizing a comprehensive MovieLens dataset and a robust implementation process, we constructed a model capable of providing tailored recommendations. Despite some discrepancies between training and validation loss, indicative of potential inconsistencies and suggesting a need for further complexity in the model, the final evaluation yielded a promising MAE of 0.9465, underscoring the model's adequate performance with scope for refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's digital age, the sheer volume of movies, TV shows, and video content available to consumers has reached unprecedented levels. Paradoxically, this wealth of options often leads to confusion and indecision among users. They can spend excessive amounts of time searching for content that aligns with their preferences, ultimately detracting from their entertainment experience. Furthermore, as online media platforms intensify their competition for user engagement, the need for personalized content recommendations becomes increasingly paramount. Without tailored suggestions, platforms risk losing their user base and, consequently, their profitability.\n",
    "\n",
    "To address these challenges, we have chosen to focus on movie recommendations. Our objective is to develop a personalized movie recommendation system that empowers users to swiftly discover content that resonates with their tastes. This not only enhances the user experience but also enables online media platforms to deliver tailored services, retain users, and boost their bottom line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comprises 1,000,209 anonymous ratings provided by 6,040 MovieLens users who joined MovieLens in 2000, reflecting a diverse set of movie preferences. The raw data is from the research Project at the University of Minnesota. The source is shown in Reference. The data consist of User ID, Movie ID, Rating, and Timestamp. User IDs range from 1 to 6040, Movie IDs from 1 to 3952, and ratings are on a 5-star scale. The timestamp is represented in seconds since the epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Define and Use Class and Function to Process the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *INFORMATION*\n",
    "\n",
    "**1. Function**\n",
    "\n",
    "* Read data and process the movie data\n",
    "* Split the data into the train(80%), validation(10%) and test(10%)\n",
    "\n",
    "**2. Class**\n",
    "\n",
    "* Class **DataSampler** about getting batched train and validation data \n",
    "* Class **DataSamplerForTest** about getting batched or all test data \n",
    "\n",
    "**3. Others**\n",
    "\n",
    "* If you want to run this script, please download [*searchparameters.py* ](https://github.com/si-tong-chen/Computational-Tool-for-Data-Science)\n",
    "* please click [*dataset*](https://github.com/si-tong-chen/Computational-Tool-for-Data-Science) to download \n",
    "* If you meet something wrong when you run this script, please keep same version as follow:\n",
    "\n",
    "       - python 3.9.11 \n",
    "       - TensorFlow==2.8.0\n",
    "       - cuda/11.4 \n",
    "       - cudnn/v8.2.2.26-prod-cuda-11.4 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Define Class and Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function about reading  and prcoessing the movie data \n",
    "\n",
    "def read_data_and_process(filname, sep=\"\\t\"):\n",
    "    col_names = [\"user\", \"item\", \"rate\", \"st\"] # 'st' is the length of the movie \n",
    "    df = pd.read_csv(filname, sep=sep, header=None, names=col_names, engine='python') \n",
    "    df[\"user\"] -= 1  # user in the raw data starts from 1\n",
    "    df[\"item\"] -= 1  # item in the raw data starts from 1\n",
    "    \n",
    "    for col in (\"user\", \"item\"): \n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    df[\"rate\"] = df[\"rate\"].astype(np.float32)\n",
    "    \n",
    "    #process the null \n",
    "    if df.isnull().any().any():\n",
    "        print(\"The document contained null values, and these null values were deleted.\")\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop=True)\n",
    "    else:\n",
    "        print(\"The document doesnot contain null values\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcation about spliting the data into the train(80%), validation(10%) and test(10%)\n",
    "\n",
    "def split_data(path):\n",
    "    df = read_data_and_process(path, sep=\"::\")\n",
    "    rows = len(df)\n",
    "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    \n",
    "    split_index_train_val = int(rows * 0.8)\n",
    "    slpit_index_val_test = int(rows*0.9)\n",
    "    df_train = df.iloc[:split_index_train_val,:]  \n",
    "    df_validation = df.iloc[split_index_train_val:slpit_index_val_test,:]\n",
    "    df_test = df.iloc[slpit_index_val_test:,:] \n",
    "\n",
    "    print('The shape of train:', df_train.shape)\n",
    "    print('The shape of validaiton:',df_validation.shape)\n",
    "    print('The shape of test:',df_test.shape)\n",
    "    \n",
    "    return df_train, df_validation,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class about get batched train data and validation data \n",
    "\n",
    "class DataSampler(object):\n",
    "    \"\"\"\n",
    "    DataSampler is used for obtaining batched train data and validation data.\n",
    "\n",
    "    Args:\n",
    "    - inputs: Input data for training or validation.\n",
    "    - batch_size: The batch size for sampling data.\n",
    "\n",
    "    Attributes:\n",
    "    - inputs: Transposed and stacked input data.\n",
    "    - batch_size: The specified batch size.\n",
    "    - num_cols: Number of columns in the input data.\n",
    "    - len: Length of the input data.\n",
    "\n",
    "    Methods:\n",
    "    - __len__: Returns the length of the input data.\n",
    "    - __iter__: Returns the data sampler itself.\n",
    "    - __next__: Returns the next batch of data.\n",
    "    - next: Returns the next batch of data.\n",
    "\n",
    "    Usage:\n",
    "    - Create an instance of DataSampler to sample batches of training or validation data.\n",
    "    - Iterate over the DataSampler to obtain batches of data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs, batch_size=64):\n",
    "        self.inputs = inputs \n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self   \n",
    "    \n",
    "    def __next__(self): \n",
    "        return self.next() \n",
    "    \n",
    "    def next(self): \n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,))\n",
    "        out = self.inputs[ids, :] \n",
    "        return [out[:, i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  class about get batched test data \n",
    "class DataSamplerForTest(DataSampler):\n",
    "    \"\"\"\n",
    "    DataSamplerForTest is used for obtaining batched test data.\n",
    "\n",
    "    Args:\n",
    "    - inputs: Input data for testing.\n",
    "    - batch_size: The batch size for testing data. If batch_size is positive, data will be split into batches.\n",
    "\n",
    "    Attributes:\n",
    "    - idx_group: List of indices indicating the data groups.\n",
    "    - group_id: Current group index.\n",
    "\n",
    "    Methods:\n",
    "    - next: Retrieves the next batch of test data.\n",
    "\n",
    "    Usage:\n",
    "    - Create an instance of DataSamplerForTest to sample test data in batches for testing purposes.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, batch_size=64):\n",
    "        super(DataSamplerForTest, self).__init__(inputs, batch_size=batch_size)        \n",
    "        \n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size))\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)] \n",
    "            \n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration \n",
    "                \n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Use Class and Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document doesnot contain null values\n",
      "The shape of train: (800167, 4)\n",
      "The shape of validaiton: (100021, 4)\n",
      "The shape of test: (100021, 4)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2000\n",
    "path = '/zhome/77/2/193848/BACT/data/movielens/ml-1m/ratings.dat'\n",
    "df_train,df_validation,df_test = split_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batched train，validation and test data\n",
    "train_data = DataSampler([df_train[\"user\"],\n",
    "                                  df_train[\"item\"],\n",
    "                                  df_train[\"rate\"]],\n",
    "                                  batch_size=batch_size)\n",
    "validation_data = DataSampler([df_validation[\"user\"],\n",
    "                                  df_validation[\"item\"],\n",
    "                                  df_validation[\"rate\"]],\n",
    "                                  batch_size=batch_size)\n",
    "test_data = DataSamplerForTest([df_test[\"user\"],\n",
    "                                   df_test[\"item\"],\n",
    "                                   df_test[\"rate\"]],\n",
    "                                   batch_size=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Model Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *INFORMATION*\n",
    "\n",
    "**1. Create SVDModel**\n",
    "\n",
    "- Create an embedding matrix encompassing and initialize all the features.\n",
    "- Construct the LFM algorithm:\n",
    "\n",
    "    $y_{pred[u, i]} = bias_{global} + bias_{user[u]} + bias_{item_[i]} + <embedding_{user[u]}, embedding_{item[i]}>$\n",
    "\n",
    "- Prepare to add regularization into the loss function:\n",
    "\n",
    "    $loss = \\sum_{u, i} |y_{pred[u, i]} - y_{true[u, i]}|^2 + \\lambda(|embedding_{user[u]}|^2 + |embedding_{item[i]}|^2)$\n",
    "\n",
    "|<img src=\"image/tf_svd_graph.png\" alt=\"Image 1\" width=\"400\" height=\"200\"> \n",
    "\n",
    "**2. Model Initialization and Setup**\n",
    "\n",
    "- Data and Problem Setup: the number of users and items dimensionality of the latent factors and the regularization strength \n",
    "- Create an instance of the SVD model \n",
    "- Define a loss function and Select an optimizer\n",
    "- Choose a performance metric\n",
    "\n",
    "\n",
    "**3. Test Model**\n",
    "\n",
    "- Test forward propagation\n",
    "\n",
    "**4. Hyperparameter Tuning**\n",
    "- Find the great hhyperparameter\n",
    "  \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "**5. Train Model**\n",
    "\n",
    "**6.Model Evaluation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Create SVDModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat SVDModel\n",
    "class SVDModel(tf.keras.Model):\n",
    "    '''\n",
    "\n",
    "    This class represents a Singular Value Decomposition (SVD) model used in the Latent Factor Model (LFM) algorithm.\n",
    "    \n",
    "    Attributes:\n",
    "    - user_num: Number of users in the dataset.\n",
    "    - item_num: Number of items (movie categories) in the dataset.\n",
    "    - dim: Dimensionality of the latent factors for users and items.\n",
    "    - reg_strength: Regularization strength for preventing overfitting.\n",
    "    \n",
    "    Methods:\n",
    "    - __init__: Initializes the SVD model with user and item embeddings, biases, and regularization.\n",
    "    - call: Defines the forward pass of the model to compute predicted ratings.\n",
    "    '''\n",
    " \n",
    "    def __init__(self, user_num, item_num, dim,reg_strength):\n",
    "        super(SVDModel, self).__init__()\n",
    "        self.reg_strength = reg_strength\n",
    "        initializer = tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "        self.user_emb = tf.keras.layers.Embedding(user_num, dim, embeddings_initializer=initializer,trainable=True)\n",
    "        self.item_emb = tf.keras.layers.Embedding(item_num, dim, embeddings_initializer=initializer,trainable=True)\n",
    "                           \n",
    "        self.global_bias = tf.Variable(initial_value=0.0, dtype=tf.float32, name=\"global_bias\")\n",
    "        self.bias_user = tf.keras.layers.Embedding(user_num, 1, embeddings_initializer='zeros', name=\"bias_user\")\n",
    "        self.bias_item = tf.keras.layers.Embedding(item_num, 1, embeddings_initializer='zeros', name=\"bias_item\")\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        Defines the forward pass of the SVD model to compute predicted ratings.\n",
    "\n",
    "        Args:\n",
    "        - inputs: A tuple of user and item indices.\n",
    "\n",
    "        Returns:\n",
    "        - output_star: Predicted ratings clipped between 1.0 and 5.0.\n",
    "        '''\n",
    "        user, item = inputs\n",
    "        user_emb = self.user_emb(user)\n",
    "        item_emb = self.item_emb(item)\n",
    "        \n",
    "        dot = tf.reduce_sum(tf.multiply(user_emb, item_emb), axis=1)\n",
    "        global_bias = self.global_bias\n",
    "        bias_user = self.bias_user(user)\n",
    "        bias_item = self.bias_item(item)\n",
    "        \n",
    "        output = dot + global_bias + tf.transpose(bias_user)+tf.transpose(bias_item)\n",
    "        output_star = tf.clip_by_value(output, 1.0, 5.0) # Clip scores, setting them to 0 if they are less than 0 and to 5 if they are greater than 5.\n",
    "        \n",
    "        reg_loss = self.reg_strength * (tf.reduce_sum(tf.square(user_emb)) + tf.reduce_sum(tf.square(item_emb))) \n",
    "        self.add_loss(reg_loss) ## perparing to add the regularization into the loss function \n",
    "\n",
    "        return output_star\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Model Initialization and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization and Setup\n",
    "user_num = 6040\n",
    "item_num = 3952\n",
    "dim  = 15\n",
    "reg_strength = 0.1\n",
    "\n",
    "model = SVDModel(user_num, item_num, dim,reg_strength)\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1, clipvalue=2.0)\n",
    "mae_metric = tf.keras.metrics.MeanAbsoluteError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Test Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Model \n",
    "user_input = tf.constant([1.0, 2.0, 3.0,4.0])\n",
    "item_input =  tf.constant([4.0, 5.0, 6.0,7.0])\n",
    "output_star = model([user_input, item_input])\n",
    "output_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Note**\n",
    "we don't run this part to search best hyperparameters in this scripts, becasue it consumes lots of time. We transfer it into .py file and run it on HPC. And then we get the best hyperparamters as follow:\n",
    "\n",
    "        - dim: 6\n",
    "        - regularization parameter: 0.1\n",
    "        - learning rate : 0.1\n",
    "        - clipvalue value:2.0\n",
    "        - best_val_loss： 2.1972108\n",
    "        - batch size: 2000\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "num_epochs =10\n",
    "num_searches =100\n",
    "dim_values =list(range(5, 15, 1))\n",
    "reg_strength_values =[1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]\n",
    "\n",
    "learning_rate_values = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]\n",
    "clipvalue_values = list(range(2,20,3))\n",
    "\n",
    "search_processor = searchparameters.SearchPara(num_epochs,num_searches,dim_values,\n",
    "                 reg_strength_values,learning_rate_values,\n",
    "                 clipvalue_values,item_num,user_num,train_data,validation_data,batch_size)\n",
    "\n",
    "best_parameterss= search_processor.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the tuned parameters, we will use these parameters to train the model with a batch size of 2000 and 500 epochs. Simultaneously, we will record the train loss and validation loss, as well as train mean absolute error (MAE) and validation MAE for each epoch. Finally, we will show it in the follow picture.\n",
    "\n",
    "\n",
    "The conclusion drawn from picture is that the training loss stabilizes around 10.8, indicating a relatively steady error in the training data. The validation loss converges around 0.98, which is relatively low, suggesting good generalization of the model to new data. The training MAE decreases from 1.1 to 0.75, demonstrating the model's increasing fit over training and a reduction in mean absolute error. The validation MAE decreases from 0.83 to around 0.75, matching the training MAE, showing consistency between the training and validation sets, and indicating that the model generalizes well to new data. \n",
    "\n",
    "However, the significant discrepancy between training and validation loss may indicate inconsistency in loss function calculation or differences in data distribution, possibly due to anomalies during data collection. Additionally, the relatively high training loss could suggest insufficient model complexity to capture all variations in the data. The stabilization of the loss and MAE over the number of training epochs indicates that beyond a certain point, the model does not significantly learn from additional training\n",
    "\n",
    "\n",
    "|<img src=\"image/loss.png\" alt=\"Image 1\" width=\"400\" height=\"200\"> | <img src=\"image/mae.png\" alt=\"Image 2\" width=\"400\" height=\"200\">|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_num = 6040\n",
    "item_num = 3952\n",
    "dim  = 6\n",
    "reg_strength = 0.1\n",
    "\n",
    "model = SVDModel(user_num, item_num, dim,reg_strength)\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1, clipvalue=2.0)\n",
    "mae_metric = tf.keras.metrics.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "draw_num = 10\n",
    "num_epochs = 500\n",
    "\n",
    "\n",
    "iter = []\n",
    "train_loss,train_mae=[],[]\n",
    "valid_loss, valid_mae=[],[]\n",
    "\n",
    "train_iter,validation_iter = [],[]\n",
    "\n",
    "train_loss_batch_record, train_mae_batch_record=[],[]\n",
    "valid_loss_batch_record, valid_mae_batch_record=[],[]\n",
    "\n",
    "## \n",
    "@tf.function\n",
    "def train_step(users, items, rates):\n",
    "    ''' \n",
    "    Perform a single training step for a recommendation model.\n",
    "        \n",
    "    Args:\n",
    "    - users: TensorFlow tensor representing user data.\n",
    "    - items: TensorFlow tensor representing item data.\n",
    "    - rates: TensorFlow tensor representing rating data.\n",
    "\n",
    "    Returns:\n",
    "    - total_loss: Total loss for the current training step.\n",
    "    - mae: Mean Absolute Error (MAE) metric for the current batch.\n",
    "    \n",
    "        \n",
    "    '''\n",
    "\n",
    "    with tf.device('/GPU:0'):  \n",
    "        with tf.GradientTape() as tape:\n",
    "            users= tf.convert_to_tensor(users)\n",
    "            items = tf.convert_to_tensor(items)\n",
    "            rates =tf.convert_to_tensor(rates)\n",
    "\n",
    "            inputs = (users, items)\n",
    "            output_star = model(inputs, training=True)\n",
    "            loss = loss_object(rates, output_star)\n",
    "            total_loss = loss + tf.reduce_sum(model.losses)       \n",
    "\n",
    "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        mae_metric.update_state(rates, output_star)\n",
    "        mae = mae_metric.result()\n",
    "\n",
    "    return total_loss, mae\n",
    "\n",
    "@tf.function \n",
    "def validation_step(users, items, rates):\n",
    "    \"\"\"\n",
    "    Perform a single validation step for a recommendation model.\n",
    "    \n",
    "    Args:\n",
    "    - users: TensorFlow tensor representing user data.\n",
    "    - items: TensorFlow tensor representing item data.\n",
    "    - rates: TensorFlow tensor representing rating data.\n",
    "\n",
    "    Returns:\n",
    "    - val_loss: Validation loss for the current validation step.\n",
    "    - val_mae: Mean Absolute Error (MAE) metric for the current batch.\n",
    "    \"\"\"\n",
    "    users= tf.convert_to_tensor(users)\n",
    "    items = tf.convert_to_tensor(items)\n",
    "    rates =tf.convert_to_tensor(rates)\n",
    "\n",
    "    inputs = (users, items)\n",
    "    model.trainable = False \n",
    "    output_star = model(inputs, training=False)\n",
    "    val_loss = loss_object(rates, output_star)\n",
    "\n",
    "    mae_metric.update_state(rates, output_star)\n",
    "    val_mae = mae_metric.result()\n",
    "\n",
    "    return val_loss, val_mae\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #train data \n",
    "    mae_metric.reset_states() \n",
    "    train_loss_batch, train_mae_batch= [], [] \n",
    "    start = time.time()\n",
    "    for i, (users, items, rates) in enumerate(train_data):\n",
    "\n",
    "\n",
    "        loss, mae = train_step(users, items, rates)\n",
    "\n",
    "        train_loss_batch.append(loss.numpy())\n",
    "        train_mae_batch.append(mae.numpy())\n",
    "     \n",
    "\n",
    "        train_loss_batch_record.append(loss.numpy())\n",
    "        train_mae_batch_record.append(mae.numpy())\n",
    "\n",
    "        train_iter.append(i+1)\n",
    "\n",
    "        if i+1 >= batch_size:\n",
    "            break\n",
    "\n",
    "    train_loss.append(np.mean(train_loss_batch))\n",
    "    train_mae.append(np.mean(train_mae_batch))\n",
    "\n",
    "    \n",
    "    #validation data\n",
    "    valid_loss_batch, valid_mae_batch = [],[]\n",
    "\n",
    "    for j, (users, items, rates) in enumerate(validation_data):\n",
    "        val_loss, val_mae = validation_step(users, items, rates)\n",
    "\n",
    "        valid_loss_batch.append(val_loss.numpy())\n",
    "        valid_mae_batch.append(val_mae.numpy())        \n",
    "\n",
    "        valid_loss_batch_record.append(val_loss.numpy())\n",
    "        valid_mae_batch_record.append(val_mae.numpy())\n",
    "\n",
    "        validation_iter.append(j+1)\n",
    "\n",
    "        if j+1 >= batch_size:\n",
    "            break \n",
    "    valid_loss.append(np.mean(valid_loss_batch))\n",
    "    valid_mae.append(np.mean(valid_mae_batch))\n",
    "\n",
    "    iter.append(epoch+1)\n",
    "\n",
    "    if epoch % draw_num == 0:\n",
    "        end = time.time()\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {np.mean(train_loss_batch)}, Train MAE = {np.mean(train_mae_batch)},Time Consuming = {round((end - start),3)}(s)\")\n",
    "        print(f\"Epoch {epoch+1}: Validation Loss = {np.mean(valid_loss_batch)}, Validation MAE = {np.mean(valid_mae_batch)},Time Consuming = {round((end - start),3)}(s)\")\n",
    "\n",
    "print(\"Finished training.\")\n",
    "# save model \n",
    "tf.saved_model.save(model, '/work3/s230027/CT/result/final_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Note**\n",
    "If you want to run the following code, please download [*'final_model'*](https://github.com/si-tong-chen/Computational-Tool-for-Data-Science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.9465680122375488\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loaded_model = tf.saved_model.load('/work3/s230027/CT/result/final_model')\n",
    "\n",
    "inference = loaded_model.signatures[\"serving_default\"]\n",
    "for user,item,rate in test_data:\n",
    "    user = tf.constant(user, dtype=tf.float32)\n",
    "    item = tf.constant(item, dtype=tf.float32)\n",
    "    rate = tf.constant(rate, dtype=tf.float32)\n",
    "\n",
    "\n",
    "output_star = inference(input_1=user,input_2=item)\n",
    "rate = tf.reshape(rate, output_star['output_1'].shape)\n",
    "mse = mean_squared_error(rate, output_star['output_1']) \n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our analysis in the third section, our movie recommendation system built using the Latent Factor Model (LFM) appears to be successful. The LFM-based movie system can provide personalized recommendation services. \n",
    "\n",
    "However, we have identified some shortcomings, such as the issue of excessive training loss and the lack of significant improvements from additional training. Due to time constraints, we are unable to make further improvements.But we have some ideas about improvements.\n",
    "\n",
    "\n",
    "Our idea is to enhance feature extraction in LFM (Latent Factor Model) by incorporating latent feature vectors corresponding to movie titles and customer movie-watching behavior. \n",
    "\n",
    "To address issues like high training loss and significant differences between training and validation losses, we propose merging the latent feature vectors generated in LFM with other user and movie attributes, such as user names, gender, age, occupation, as well as movie genres and duration. These merged attributes would be fed into a Multi-Layer Perceptron (MLP) or other types of neural networks to improve the model's performance and generalization capability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
